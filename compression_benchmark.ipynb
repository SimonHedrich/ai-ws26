{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19c3b258",
   "metadata": {},
   "source": [
    "# YOLO Compression Benchmark on MOT17\n",
    "\n",
    "Benchmarks **inference speed** and **detection quality** for three compression\n",
    "strategies applied to YOLO person detectors, all evaluated on the same\n",
    "MOT17-02-SDP pedestrian tracking sequence.\n",
    "\n",
    "## Experiments\n",
    "| # | Experiment | Description |\n",
    "|---|------------|-------------|\n",
    "| 1 | **Baseline** | YOLOv8 / YOLO11 / YOLO26 × nano/small/medium — raw comparison |\n",
    "| 2 | **Quantization** | yolov8m: FP32 → ONNX FP32 → ONNX FP16 → TRT variants (Jetson) |\n",
    "| 3 | **Structured pruning** | yolov8m: L1 channel pruning at 0 %, 30 %, 50 % sparsity |\n",
    "\n",
    "## Metrics\n",
    "| Metric | Description |\n",
    "|--------|-------------|\n",
    "| `mean_ms` | Mean inference time per image (ms) |\n",
    "| `std_ms` | Standard deviation of inference time (ms) |\n",
    "| `total_s` | Total benchmark wall time (s) |\n",
    "| `f1` | F1 score @ IoU = 0.5 vs. MOT17 ground truth |\n",
    "\n",
    "> **Platform note** — TRT experiments require CUDA (Jetson).\n",
    "> ONNX FP16 uses `CUDAExecutionProvider` on Jetson (full fp16 speedup) and falls back\n",
    "> to `CPUExecutionProvider` on CPU where ONNXRuntime may upcast to fp32 internally.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0283695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch        : 2.10.0\n",
      "Auto device    : mps\n",
      "CUDA available : False\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "plt.rcParams.update({\n",
    "    \"figure.dpi\": 120,\n",
    "    \"font.size\": 11,\n",
    "    \"axes.spines.top\": False,\n",
    "    \"axes.spines.right\": False,\n",
    "})\n",
    "\n",
    "_dev_auto = \"0\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"PyTorch        : {torch.__version__}\")\n",
    "print(f\"Auto device    : {_dev_auto}\")\n",
    "print(f\"CUDA available : {torch.cuda.is_available()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9e9455a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CONFIG — edit here to adapt to your setup\n",
    "# ============================================================\n",
    "\n",
    "# Single MOT17 sequence used for ALL experiments\n",
    "SEQUENCE    = \"data/MOT17/train/MOT17-02-SDP\"\n",
    "\n",
    "# Inference device: \"cpu\" | \"0\" (first CUDA GPU on Jetson)\n",
    "DEVICE      = _dev_auto\n",
    "\n",
    "# Warm-up frames discarded before timing starts\n",
    "WARMUP      = 10\n",
    "\n",
    "# Max frames per run (keeps notebook fast; set None for all 600)\n",
    "FRAME_LIMIT = 200\n",
    "\n",
    "# Common inference settings\n",
    "IMGSZ       = 640\n",
    "CONF        = 0.25\n",
    "\n",
    "# IoU threshold used for GT matching (detection quality)\n",
    "IOU_THRESH  = 0.5\n",
    "\n",
    "# Nine baseline models (3 versions × 3 sizes)\n",
    "MODELS = [\n",
    "    \"yolov8n.pt\", \"yolov8s.pt\", \"yolov8m.pt\",\n",
    "    \"yolo11n.pt\", \"yolo11s.pt\", \"yolo11m.pt\",\n",
    "    \"yolo26n.pt\", \"yolo26s.pt\", \"yolo26m.pt\",\n",
    "]\n",
    "\n",
    "# Pruning sparsity ratios to evaluate\n",
    "PRUNE_RATIOS = [0.0, 0.3, 0.5]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7014b6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Frame collection ──────────────────────────────────────────────────────\n",
    "\n",
    "def collect_frames(sequence_dir: str, limit: int | None = None) -> list[str]:\n",
    "    \"\"\"Return sorted list of .jpg frame paths from a single MOT17 sequence.\"\"\"\n",
    "    pattern = str(Path(sequence_dir) / \"img1\" / \"*.jpg\")\n",
    "    frames = sorted(glob(pattern))\n",
    "    if not frames:\n",
    "        raise FileNotFoundError(f\"No frames found at {pattern!r}\")\n",
    "    if limit:\n",
    "        frames = frames[:limit]\n",
    "    return frames\n",
    "\n",
    "\n",
    "# ── Ground-truth loading ──────────────────────────────────────────────────\n",
    "\n",
    "def load_gt(sequence_dir: str) -> dict[int, list]:\n",
    "    \"\"\"\n",
    "    Parse MOT17 gt.txt.\n",
    "    Format: frame, track_id, x, y, w, h, conf, class_id, visibility\n",
    "      conf == 1  → annotation is active in this frame\n",
    "      class_id == 1  → pedestrian (the only class YOLO detects here)\n",
    "    Returns {frame_id: [[x1,y1,x2,y2], ...]}\n",
    "    \"\"\"\n",
    "    gt_path = Path(sequence_dir) / \"gt\" / \"gt.txt\"\n",
    "    gt: dict[int, list] = {}\n",
    "    with open(gt_path) as fh:\n",
    "        for line in fh:\n",
    "            p = line.strip().split(\",\")\n",
    "            frame_id  = int(p[0])\n",
    "            conf_flag = int(p[6])      # 1 = active\n",
    "            class_id  = int(p[7])      # 1 = pedestrian\n",
    "            if conf_flag != 1 or class_id != 1:\n",
    "                continue\n",
    "            x, y, w, h = float(p[2]), float(p[3]), float(p[4]), float(p[5])\n",
    "            if w <= 0 or h <= 0:\n",
    "                continue\n",
    "            gt.setdefault(frame_id, []).append([x, y, x + w, y + h])\n",
    "    return gt\n",
    "\n",
    "\n",
    "# ── IoU & matching ────────────────────────────────────────────────────────\n",
    "\n",
    "def _box_iou(b1: list, b2: list) -> float:\n",
    "    \"\"\"Axis-aligned IoU for two boxes in xyxy format.\"\"\"\n",
    "    xi1 = max(b1[0], b2[0]); yi1 = max(b1[1], b2[1])\n",
    "    xi2 = min(b1[2], b2[2]); yi2 = min(b1[3], b2[3])\n",
    "    inter = max(0.0, xi2 - xi1) * max(0.0, yi2 - yi1)\n",
    "    a1 = (b1[2] - b1[0]) * (b1[3] - b1[1])\n",
    "    a2 = (b2[2] - b2[0]) * (b2[3] - b2[1])\n",
    "    union = a1 + a2 - inter\n",
    "    return inter / union if union > 0 else 0.0\n",
    "\n",
    "\n",
    "def _match(pred_boxes: np.ndarray, gt_boxes: list, iou_thresh: float) -> tuple[int, int, int]:\n",
    "    \"\"\"Greedy IoU matching → (TP, FP, FN).\"\"\"\n",
    "    n_gt, n_pred = len(gt_boxes), len(pred_boxes)\n",
    "    if n_gt == 0 and n_pred == 0:\n",
    "        return 0, 0, 0\n",
    "    if n_gt == 0:\n",
    "        return 0, n_pred, 0\n",
    "    if n_pred == 0:\n",
    "        return 0, 0, n_gt\n",
    "\n",
    "    matched_gt: set[int] = set()\n",
    "    tp = 0\n",
    "    for pred in pred_boxes:\n",
    "        best_iou, best_j = iou_thresh, -1\n",
    "        for j, gt in enumerate(gt_boxes):\n",
    "            if j in matched_gt:\n",
    "                continue\n",
    "            iou = _box_iou(pred.tolist(), gt)\n",
    "            if iou > best_iou:\n",
    "                best_iou, best_j = iou, j\n",
    "        if best_j >= 0:\n",
    "            tp += 1\n",
    "            matched_gt.add(best_j)\n",
    "\n",
    "    return tp, n_pred - tp, n_gt - tp\n",
    "\n",
    "\n",
    "# ── Core benchmark routine ────────────────────────────────────────────────\n",
    "\n",
    "def run_benchmark(\n",
    "    model: YOLO,\n",
    "    frames: list[str],\n",
    "    gt_by_frame: dict,\n",
    "    imgsz: int = IMGSZ,\n",
    "    conf: float = CONF,\n",
    "    warmup: int = WARMUP,\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Warm-up then time frame-by-frame inference.\n",
    "    Simultaneously accumulates TP/FP/FN against MOT17 ground truth.\n",
    "\n",
    "    Returns a dict with keys:\n",
    "        mean_ms, std_ms, total_s   – timing\n",
    "        det_precision, det_recall, f1  – detection quality\n",
    "        n_images                   – number of benchmarked frames\n",
    "    \"\"\"\n",
    "    assert len(frames) > warmup, f\"Need > {warmup} frames (got {len(frames)}).\"\n",
    "\n",
    "    # Warm-up (not timed)\n",
    "    for img in frames[:warmup]:\n",
    "        model.predict(img, imgsz=imgsz, conf=conf, device=DEVICE, verbose=False)\n",
    "\n",
    "    bench = frames[warmup:]\n",
    "    times_ms: list[float] = []\n",
    "    tp_total = fp_total = fn_total = 0\n",
    "\n",
    "    for img_path in bench:\n",
    "        frame_id = int(Path(img_path).stem)\n",
    "        gt_boxes = gt_by_frame.get(frame_id, [])\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        results = model.predict(img_path, imgsz=imgsz, conf=conf, device=DEVICE, verbose=False)\n",
    "        times_ms.append((time.perf_counter() - t0) * 1e3)\n",
    "\n",
    "        boxes_obj = results[0].boxes\n",
    "        if boxes_obj is not None and len(boxes_obj):\n",
    "            cls  = boxes_obj.cls.cpu().numpy()\n",
    "            xyxy = boxes_obj.xyxy.cpu().numpy()\n",
    "            pred_boxes = xyxy[cls == 0]          # COCO class 0 = person\n",
    "        else:\n",
    "            pred_boxes = np.zeros((0, 4))\n",
    "\n",
    "        tp, fp, fn = _match(pred_boxes, gt_boxes, IOU_THRESH)\n",
    "        tp_total += tp; fp_total += fp; fn_total += fn\n",
    "\n",
    "    mean_ms = float(np.mean(times_ms))\n",
    "    std_ms  = float(np.std(times_ms))\n",
    "    total_s = sum(times_ms) / 1000.0\n",
    "\n",
    "    denom_p = tp_total + fp_total\n",
    "    denom_r = tp_total + fn_total\n",
    "    det_prec = tp_total / denom_p if denom_p > 0 else 0.0\n",
    "    det_rec  = tp_total / denom_r if denom_r > 0 else 0.0\n",
    "    f1 = (2 * det_prec * det_rec / (det_prec + det_rec)\n",
    "          if (det_prec + det_rec) > 0 else 0.0)\n",
    "\n",
    "    return dict(\n",
    "        mean_ms=round(mean_ms, 3),\n",
    "        std_ms=round(std_ms, 3),\n",
    "        total_s=round(total_s, 3),\n",
    "        det_precision=round(det_prec, 4),\n",
    "        det_recall=round(det_rec, 4),\n",
    "        f1=round(f1, 4),\n",
    "        n_images=len(bench),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357e223e",
   "metadata": {},
   "source": [
    "---\n",
    "## Data\n",
    "\n",
    "All experiments use the same 200-frame window from **MOT17-02-SDP**\n",
    "(600 frames total; capped at `FRAME_LIMIT` for speed).\n",
    "Ground-truth boxes are filtered to active pedestrian annotations (`conf=1, class=1`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34f74275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence   : MOT17-02-SDP\n",
      "Frames     : 200 total  |  10 warm-up  |  190 benchmarked\n",
      "GT boxes   : 5354 pedestrian boxes across 200 frames\n"
     ]
    }
   ],
   "source": [
    "frames      = collect_frames(SEQUENCE, limit=FRAME_LIMIT)\n",
    "gt_by_frame = load_gt(SEQUENCE)\n",
    "\n",
    "n_bench = len(frames) - WARMUP\n",
    "gt_frames_in_bench = {fid: boxes for fid, boxes in gt_by_frame.items()\n",
    "                      if fid <= int(Path(frames[-1]).stem)}\n",
    "n_gt_boxes = sum(len(v) for v in gt_frames_in_bench.values())\n",
    "\n",
    "print(f\"Sequence   : {Path(SEQUENCE).name}\")\n",
    "print(f\"Frames     : {len(frames)} total  |  {WARMUP} warm-up  |  {n_bench} benchmarked\")\n",
    "print(f\"GT boxes   : {n_gt_boxes} pedestrian boxes across \"\n",
    "      f\"{len(gt_frames_in_bench)} frames\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aac072",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 1 — Baseline Model Comparison\n",
    "\n",
    "All nine models are evaluated with default settings (`imgsz=640`, `conf=0.25`).\n",
    "\n",
    "| Version | nano | small | medium |\n",
    "|---------|------|-------|--------|\n",
    "| YOLOv8  | yolov8n | yolov8s | yolov8m |\n",
    "| YOLO11  | yolo11n | yolo11s | yolo11m |\n",
    "| YOLO26  | yolo26n | yolo26s | yolo26m |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938cded9",
   "metadata": {},
   "outputs": [],
   "source": "rows_baseline = []\n\nfor model_name in MODELS:\n    print(f\"  {model_name:<14}\", end=\" \", flush=True)\n    try:\n        yolo = YOLO(model_name)\n        params_m = sum(p.numel() for p in yolo.model.parameters()) / 1e6\n        m = run_benchmark(yolo, frames, gt_by_frame)\n        rows_baseline.append({\"model\": model_name, \"params_M\": round(params_m, 2), **m})\n        print(f\"{m['mean_ms']:>6.1f} ± {m['std_ms']:>4.1f} ms   \"\n              f\"F1={m['f1']:.3f}   {params_m:.1f}M params\")\n    except Exception as exc:\n        print(f\"SKIP: {exc}\")\n    finally:\n        try:\n            del yolo\n        except NameError:\n            pass\n        gc.collect()\n        if torch.cuda.is_available(): torch.cuda.empty_cache()\n\ndf_baseline = pd.DataFrame(rows_baseline)\nPath(\"results\").mkdir(exist_ok=True)\nif df_baseline.empty:\n    print(\"\\n[ERROR] No results — all models failed. Check DEVICE and re-run.\")\nelse:\n    df_baseline.to_csv(\"results/baseline_nb.csv\", index=False)\n    display(df_baseline[[\"model\",\"params_M\",\"mean_ms\",\"std_ms\",\"total_s\",\"det_precision\",\"det_recall\",\"f1\"]])\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729d5f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_baseline.empty:\n",
    "    print(\"No baseline results to plot.\")\n",
    "else:\n",
    "    _PALETTE = {\"yolov8\": \"#4C72B0\", \"yolo11\": \"#DD8452\", \"yolo26\": \"#55A868\"}\n",
    "    \n",
    "    def _bar_color(model_name: str) -> str:\n",
    "        for key, col in _PALETTE.items():\n",
    "            if key in model_name:\n",
    "                return col\n",
    "        return \"#888\"\n",
    "    \n",
    "    labels = [m.replace(\".pt\", \"\") for m in df_baseline[\"model\"]]\n",
    "    bar_cols = [_bar_color(m) for m in df_baseline[\"model\"]]\n",
    "    legend_patches = [mpatches.Patch(color=c, label=v) for v, c in _PALETTE.items()]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(17, 5))\n",
    "    fig.suptitle(\"Experiment 1 — Baseline Model Comparison\", fontsize=14, fontweight=\"bold\")\n",
    "    \n",
    "    # ── Speed ──────────────────────────────────────────────────────────────────\n",
    "    ax = axes[0]\n",
    "    ax.bar(labels, df_baseline[\"mean_ms\"], color=bar_cols,\n",
    "           yerr=df_baseline[\"std_ms\"], capsize=4, alpha=0.85, ecolor=\"#444\")\n",
    "    ax.set_ylabel(\"Mean inference time (ms / img)\")\n",
    "    ax.set_title(\"Inference Speed\")\n",
    "    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    ax.legend(handles=legend_patches, fontsize=9)\n",
    "    \n",
    "    # ── F1 score ───────────────────────────────────────────────────────────────\n",
    "    ax = axes[1]\n",
    "    ax.bar(labels, df_baseline[\"f1\"], color=bar_cols, alpha=0.85)\n",
    "    ax.set_ylabel(\"F1 @ IoU = 0.5\")\n",
    "    ax.set_title(\"Detection Quality\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    ax.legend(handles=legend_patches, fontsize=9)\n",
    "    \n",
    "    # ── Speed–accuracy scatter ─────────────────────────────────────────────────\n",
    "    ax = axes[2]\n",
    "    for version, color in _PALETTE.items():\n",
    "        mask = df_baseline[\"model\"].str.contains(version)\n",
    "        sub  = df_baseline[mask]\n",
    "        ax.scatter(sub[\"mean_ms\"], sub[\"f1\"], c=color, s=90, label=version, zorder=3)\n",
    "        for _, r in sub.iterrows():\n",
    "            size_tag = r[\"model\"].replace(\".pt\", \"\").replace(version, \"\")[-1]  # n/s/m\n",
    "            ax.annotate(f\" {size_tag}\", (r[\"mean_ms\"], r[\"f1\"]),\n",
    "                        fontsize=8, va=\"center\")\n",
    "    ax.set_xlabel(\"Mean inference time (ms / img)\")\n",
    "    ax.set_ylabel(\"F1 @ IoU = 0.5\")\n",
    "    ax.set_title(\"Speed–Accuracy Trade-off\")\n",
    "    ax.legend(fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"results/baseline_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Saved → results/baseline_comparison.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3ab25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_baseline.empty:\n",
    "    print(\"No baseline results to plot.\")\n",
    "else:\n",
    "    fig, ax = plt.subplots(figsize=(10, 4))\n",
    "    ax.bar(labels, df_baseline[\"params_M\"], color=bar_cols, alpha=0.85)\n",
    "    ax.set_ylabel(\"Parameters (M)\")\n",
    "    ax.set_title(\"Model Size (Parameter Count)\")\n",
    "    ax.set_xticklabels(labels, rotation=45, ha=\"right\")\n",
    "    ax.legend(handles=legend_patches, fontsize=9)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"results/baseline_params.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c334920",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 2 — Quantization\n",
    "\n",
    "Tests **yolov8m** at progressively reduced numerical precision.\n",
    "\n",
    "| Format | Runtime | Available |\n",
    "|--------|---------|-----------|\n",
    "| `fp32` | PyTorch | ✓ always |\n",
    "| `onnx_fp32` | ONNXRuntime | ✓ CPU + Jetson |\n",
    "| `onnx_fp16` | ONNXRuntime | ✓ CPU (may upcast) + Jetson CUDA (full fp16) |\n",
    "| `trt_fp16` | TensorRT | ✗ Jetson only |\n",
    "| `trt_int8` | TensorRT INT8 | ✗ Jetson only |\n",
    "\n",
    "F1 is computed for all YOLO-loaded formats. Raw ONNX-only paths skip F1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ec4195",
   "metadata": {},
   "outputs": [],
   "source": "import gc\n\nrows_quant = []\nQUANT_MODEL = \"yolov8m.pt\"\n\n# ── 1. FP32 PyTorch baseline ───────────────────────────────────────────────\nprint(\"[fp32]\", flush=True)\nyolo_fp32 = YOLO(QUANT_MODEL)\nm = run_benchmark(yolo_fp32, frames, gt_by_frame)\nrows_quant.append({\"format\": \"fp32\", **m})\nprint(f\"  {m['mean_ms']:.1f} ± {m['std_ms']:.1f} ms   F1={m['f1']:.3f}\")\ndel yolo_fp32; gc.collect()\nif torch.cuda.is_available(): torch.cuda.empty_cache()\n\n# ── 2. ONNX FP32 (measures ONNXRuntime overhead vs PyTorch) ───────────────\nprint(\"\\n[onnx_fp32]\", flush=True)\n_onnx_fp32 = Path(\"results/quant_fp32.onnx\")\ntry:\n    if not _onnx_fp32.exists():\n        YOLO(QUANT_MODEL).export(format=\"onnx\", half=False,\n                                  imgsz=IMGSZ, dynamic=False, verbose=False)\n        Path(QUANT_MODEL.replace(\".pt\", \".onnx\")).rename(_onnx_fp32)\n    yolo_onnx_fp32 = YOLO(str(_onnx_fp32))\n    m = run_benchmark(yolo_onnx_fp32, frames, gt_by_frame)\n    rows_quant.append({\"format\": \"onnx_fp32\", **m})\n    print(f\"  {m['mean_ms']:.1f} ± {m['std_ms']:.1f} ms   F1={m['f1']:.3f}\")\n    del yolo_onnx_fp32\nexcept Exception as exc:\n    print(f\"  SKIP: {exc}\")\nfinally:\n    gc.collect()\n    if torch.cuda.is_available(): torch.cuda.empty_cache()\n\n# ── 3. ONNX FP16 (half-precision weights; ort CPU may upcast internally) ──\nprint(\"\\n[onnx_fp16]\", flush=True)\n_onnx_fp16 = Path(\"results/quant_fp16.onnx\")\ntry:\n    if not _onnx_fp16.exists():\n        YOLO(QUANT_MODEL).export(format=\"onnx\", half=True,\n                                  imgsz=IMGSZ, dynamic=False, verbose=False)\n        Path(QUANT_MODEL.replace(\".pt\", \".onnx\")).rename(_onnx_fp16)\n    yolo_onnx_fp16 = YOLO(str(_onnx_fp16))\n    m = run_benchmark(yolo_onnx_fp16, frames, gt_by_frame)\n    rows_quant.append({\"format\": \"onnx_fp16\", **m})\n    print(f\"  {m['mean_ms']:.1f} ± {m['std_ms']:.1f} ms   F1={m['f1']:.3f}\")\n    del yolo_onnx_fp16\nexcept Exception as exc:\n    print(f\"  SKIP: {exc}\")\nfinally:\n    gc.collect()\n    if torch.cuda.is_available(): torch.cuda.empty_cache()\n\n# ── 4. TRT variants (CUDA / Jetson only) ──────────────────────────────────\nif torch.cuda.is_available():\n    for label, half, int8 in [(\"trt_fp16\", True, False), (\"trt_int8\", False, True)]:\n        print(f\"\\n[{label}]\", flush=True)\n        _eng = Path(f\"results/quant_{label}.engine\")\n        try:\n            if not _eng.exists():\n                YOLO(QUANT_MODEL).export(format=\"engine\", imgsz=IMGSZ,\n                                         half=half, int8=int8,\n                                         data=\"config/mot17.yaml\", verbose=False)\n                Path(QUANT_MODEL.replace(\".pt\", \".engine\")).rename(_eng)\n            yolo_trt = YOLO(str(_eng))\n            m = run_benchmark(yolo_trt, frames, gt_by_frame)\n            rows_quant.append({\"format\": label, **m})\n            print(f\"  {m['mean_ms']:.1f} ± {m['std_ms']:.1f} ms   F1={m['f1']:.3f}\")\n            del yolo_trt\n        except Exception as exc:\n            print(f\"  SKIP: {exc}\")\n        finally:\n            gc.collect()\n            torch.cuda.empty_cache()\nelse:\n    print(\"\\n[INFO] No CUDA → trt_fp16 and trt_int8 skipped.\")\n    print(\"       Run on Jetson with DEVICE='0' to include TRT results.\")\n\ndf_quant = pd.DataFrame(rows_quant)\nif df_quant.empty:\n    print(\"\\n[ERROR] No quantization results recorded.\")\nelse:\n    df_quant.to_csv(\"results/quantized_nb.csv\", index=False)\n    display(df_quant[[\"format\",\"mean_ms\",\"std_ms\",\"total_s\",\"f1\"]])\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36728e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_quant.empty:\n",
    "    print(\"No quantization results to plot.\")\n",
    "else:\n",
    "    _q_colors = {\n",
    "        \"fp32\":       \"#4C72B0\",\n",
    "        \"onnx_fp32\":  \"#6fa8dc\",\n",
    "        \"onnx_fp16\":  \"#4db6ac\",\n",
    "        \"trt_fp16\":   \"#ff7043\",\n",
    "        \"trt_int8\":   \"#e53935\",\n",
    "    }\n",
    "    \n",
    "    present = df_quant[\"format\"].tolist()\n",
    "    q_cols  = [_q_colors.get(f, \"#aaa\") for f in present]\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    fig.suptitle(\"Experiment 2 — Quantization (yolov8m)\", fontsize=14, fontweight=\"bold\")\n",
    "    \n",
    "    # ── Speed ──────────────────────────────────────────────────────────────────\n",
    "    ax = axes[0]\n",
    "    ax.bar(present, df_quant[\"mean_ms\"], color=q_cols, alpha=0.85,\n",
    "           yerr=df_quant[\"std_ms\"], capsize=5, ecolor=\"#444\")\n",
    "    ax.set_ylabel(\"Mean inference time (ms / img)\")\n",
    "    ax.set_title(\"Inference Speed\")\n",
    "    ax.set_xticklabels(present, rotation=30, ha=\"right\")\n",
    "    # Annotate speedup relative to fp32\n",
    "    fp32_ms = df_quant.loc[df_quant[\"format\"] == \"fp32\", \"mean_ms\"].values\n",
    "    if len(fp32_ms):\n",
    "        for i, (_, row) in enumerate(df_quant.iterrows()):\n",
    "            ratio = fp32_ms[0] / row[\"mean_ms\"] if row[\"mean_ms\"] > 0 else 1\n",
    "            ax.text(i, row[\"mean_ms\"] + df_quant[\"std_ms\"].max() * 0.1,\n",
    "                    f\"{ratio:.2f}×\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "    \n",
    "    # ── F1 ─────────────────────────────────────────────────────────────────────\n",
    "    ax = axes[1]\n",
    "    f1_vals = df_quant[\"f1\"].fillna(0)\n",
    "    ax.bar(present, f1_vals, color=q_cols, alpha=0.85)\n",
    "    ax.set_ylabel(\"F1 @ IoU = 0.5\")\n",
    "    ax.set_title(\"Detection Quality\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_xticklabels(present, rotation=30, ha=\"right\")\n",
    "    ax.axhline(float(df_quant.loc[df_quant[\"format\"] == \"fp32\", \"f1\"].values[0]),\n",
    "               color=\"red\", linestyle=\"--\", linewidth=1, label=\"FP32 baseline\")\n",
    "    ax.legend(fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"results/quantization_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Saved → results/quantization_comparison.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3059d48c",
   "metadata": {},
   "source": [
    "---\n",
    "## Experiment 3 — Structured Pruning\n",
    "\n",
    "Applies **L1-norm channel pruning** (`torch-pruning` library) to yolov8m at\n",
    "three sparsity ratios.  No fine-tuning is performed — this isolates the\n",
    "raw effect of pruning on both speed and accuracy.\n",
    "\n",
    "| Ratio | Effect |\n",
    "|-------|--------|\n",
    "| 0 % | Baseline — no pruning |\n",
    "| 30 % | 30 % of channels removed |\n",
    "| 50 % | 50 % of channels removed |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b6f912",
   "metadata": {},
   "outputs": [],
   "source": "import gc\n\ntry:\n    import torch_pruning as tp\n    _tp_available = True\nexcept ImportError:\n    print(\"torch-pruning not installed — pip install torch-pruning\")\n    _tp_available = False\n\nPRUNE_MODEL = \"yolov8m.pt\"\n\n\ndef _apply_pruning(torch_model: torch.nn.Module, ratio: float) -> torch.nn.Module:\n    \"\"\"L1 structured channel pruning at the given sparsity ratio.\"\"\"\n    example_input = torch.randn(1, 3, IMGSZ, IMGSZ)\n\n    # Protect the detection head layers from being pruned\n    ignored = []\n    for m in torch_model.modules():\n        if isinstance(m, torch.nn.Linear):\n            ignored.append(m)\n        if hasattr(m, \"cv3\") or hasattr(m, \"dfl\"):\n            ignored.append(m)\n\n    pruner = tp.pruner.MagnitudePruner(\n        torch_model,\n        example_input,\n        importance=tp.importance.MagnitudeImportance(p=1),\n        iterative_steps=1,\n        pruning_ratio=ratio,\n        ignored_layers=ignored,\n    )\n    pruner.step()\n    return torch_model\n\n\nrows_pruning = []\n\nif _tp_available:\n    for ratio in PRUNE_RATIOS:\n        label = f\"{int(ratio * 100)}%\"\n        print(f\"[ratio={label}]\", end=\" \", flush=True)\n        try:\n            yolo_p = YOLO(PRUNE_MODEL)\n            params_before = sum(p.numel() for p in yolo_p.model.parameters()) / 1e6\n\n            if ratio > 0.0:\n                yolo_p.model = _apply_pruning(yolo_p.model, ratio)\n\n            params_after = sum(p.numel() for p in yolo_p.model.parameters()) / 1e6\n            m = run_benchmark(yolo_p, frames, gt_by_frame)\n\n            rows_pruning.append({\n                \"ratio\":           ratio,\n                \"label\":           label,\n                \"params_before_M\": round(params_before, 2),\n                \"params_after_M\":  round(params_after, 2),\n                **m,\n            })\n            print(f\"{m['mean_ms']:.1f} ± {m['std_ms']:.1f} ms   \"\n                  f\"F1={m['f1']:.3f}   \"\n                  f\"{params_before:.1f}M → {params_after:.1f}M params\")\n        except Exception as exc:\n            print(f\"SKIP: {exc}\")\n        finally:\n            try:\n                del yolo_p\n            except NameError:\n                pass\n            gc.collect()\n            if torch.cuda.is_available(): torch.cuda.empty_cache()\nelse:\n    print(\"Pruning skipped.\")\n\ndf_pruning = pd.DataFrame(rows_pruning) if rows_pruning else pd.DataFrame()\nif not df_pruning.empty:\n    df_pruning.to_csv(\"results/pruning_nb.csv\", index=False)\n    display(df_pruning[[\"label\",\"params_before_M\",\"params_after_M\",\n                         \"mean_ms\",\"std_ms\",\"total_s\",\"f1\"]])\n"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d559a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not df_pruning.empty:\n",
    "    _p_colors = [\"#4C72B0\", \"#DD8452\", \"#C44E52\"]\n",
    "    labels_p  = df_pruning[\"label\"].tolist()\n",
    "    p_cols    = _p_colors[:len(labels_p)]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "    fig.suptitle(\"Experiment 3 — Structured Pruning (yolov8m)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "    # ── Speed ────────────────────────────────────────────────────────────────\n",
    "    ax = axes[0]\n",
    "    ax.bar(labels_p, df_pruning[\"mean_ms\"], color=p_cols, alpha=0.85,\n",
    "           yerr=df_pruning[\"std_ms\"], capsize=5, ecolor=\"#444\")\n",
    "    ax.set_ylabel(\"Mean inference time (ms / img)\")\n",
    "    ax.set_title(\"Inference Speed\")\n",
    "    # Annotate speedup vs baseline (0 %)\n",
    "    base_ms = df_pruning[\"mean_ms\"].iloc[0]\n",
    "    for i, row in df_pruning.iterrows():\n",
    "        ratio_ = base_ms / row[\"mean_ms\"] if row[\"mean_ms\"] > 0 else 1\n",
    "        ax.text(i, row[\"mean_ms\"] + df_pruning[\"std_ms\"].max() * 0.1,\n",
    "                f\"{ratio_:.2f}×\", ha=\"center\", va=\"bottom\", fontsize=9)\n",
    "\n",
    "    # ── F1 ───────────────────────────────────────────────────────────────────\n",
    "    ax = axes[1]\n",
    "    ax.bar(labels_p, df_pruning[\"f1\"], color=p_cols, alpha=0.85)\n",
    "    ax.set_ylabel(\"F1 @ IoU = 0.5\")\n",
    "    ax.set_title(\"Detection Quality\")\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.axhline(float(df_pruning[\"f1\"].iloc[0]),\n",
    "               color=\"red\", linestyle=\"--\", linewidth=1, label=\"Unpruned baseline\")\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "    # ── Params ───────────────────────────────────────────────────────────────\n",
    "    ax = axes[2]\n",
    "    x = range(len(labels_p))\n",
    "    w = 0.35\n",
    "    ax.bar([xi - w / 2 for xi in x], df_pruning[\"params_before_M\"],\n",
    "           width=w, alpha=0.6, label=\"Before pruning\", color=\"#4C72B0\")\n",
    "    ax.bar([xi + w / 2 for xi in x], df_pruning[\"params_after_M\"],\n",
    "           width=w, alpha=0.85, label=\"After pruning\", color=\"#DD8452\")\n",
    "    ax.set_xticks(list(x)); ax.set_xticklabels(labels_p)\n",
    "    ax.set_ylabel(\"Parameters (M)\")\n",
    "    ax.set_title(\"Parameter Reduction\")\n",
    "    ax.legend(fontsize=9)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"results/pruning_comparison.png\", dpi=150, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"Saved → results/pruning_comparison.png\")\n",
    "else:\n",
    "    print(\"No pruning results to plot.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f387b7bb",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "The table below merges all three experiments into a single view ordered by\n",
    "`mean_ms` (fastest first).  Use it to compare the speed–accuracy frontier\n",
    "across model families, quantization formats, and pruning levels.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a794531b",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_rows = []\n",
    "\n",
    "# Baseline models\n",
    "for _, r in df_baseline.iterrows():\n",
    "    summary_rows.append({\n",
    "        \"experiment\": \"baseline\",\n",
    "        \"label\":      r[\"model\"].replace(\".pt\", \"\"),\n",
    "        \"params_M\":   r[\"params_M\"],\n",
    "        \"mean_ms\":    r[\"mean_ms\"],\n",
    "        \"std_ms\":     r[\"std_ms\"],\n",
    "        \"total_s\":    r[\"total_s\"],\n",
    "        \"f1\":         r[\"f1\"],\n",
    "    })\n",
    "\n",
    "# Quantization\n",
    "for _, r in df_quant.iterrows():\n",
    "    summary_rows.append({\n",
    "        \"experiment\": \"quantization\",\n",
    "        \"label\":      f\"yolov8m [{r['format']}]\",\n",
    "        \"params_M\":   None,\n",
    "        \"mean_ms\":    r[\"mean_ms\"],\n",
    "        \"std_ms\":     r[\"std_ms\"],\n",
    "        \"total_s\":    r[\"total_s\"],\n",
    "        \"f1\":         r.get(\"f1\", None),\n",
    "    })\n",
    "\n",
    "# Pruning\n",
    "if not df_pruning.empty:\n",
    "    for _, r in df_pruning.iterrows():\n",
    "        summary_rows.append({\n",
    "            \"experiment\": \"pruning\",\n",
    "            \"label\":      f\"yolov8m pruned {r['label']}\",\n",
    "            \"params_M\":   r[\"params_after_M\"],\n",
    "            \"mean_ms\":    r[\"mean_ms\"],\n",
    "            \"std_ms\":     r[\"std_ms\"],\n",
    "            \"total_s\":    r[\"total_s\"],\n",
    "            \"f1\":         r[\"f1\"],\n",
    "        })\n",
    "\n",
    "df_summary = pd.DataFrame(summary_rows).sort_values(\"mean_ms\").reset_index(drop=True)\n",
    "df_summary.to_csv(\"results/summary_nb.csv\", index=False)\n",
    "df_summary.style.background_gradient(subset=[\"mean_ms\"], cmap=\"RdYlGn_r\") \\\n",
    "               .background_gradient(subset=[\"f1\"],      cmap=\"RdYlGn\") \\\n",
    "               .format(precision=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1174fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "_exp_colors = {\n",
    "    \"baseline\":     \"#4C72B0\",\n",
    "    \"quantization\": \"#DD8452\",\n",
    "    \"pruning\":      \"#55A868\",\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(11, 6))\n",
    "fig.suptitle(\"Speed–Accuracy Frontier — All Experiments\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "for exp, grp in df_summary.groupby(\"experiment\"):\n",
    "    valid = grp.dropna(subset=[\"f1\"])\n",
    "    ax.scatter(valid[\"mean_ms\"], valid[\"f1\"],\n",
    "               c=_exp_colors.get(exp, \"#888\"), s=90, label=exp,\n",
    "               alpha=0.85, zorder=3)\n",
    "    for _, r in valid.iterrows():\n",
    "        ax.annotate(f\"  {r['label']}\", (r[\"mean_ms\"], r[\"f1\"]),\n",
    "                    fontsize=7, va=\"center\", alpha=0.8)\n",
    "\n",
    "ax.set_xlabel(\"Mean inference time (ms / img)\", fontsize=12)\n",
    "ax.set_ylabel(\"F1 @ IoU = 0.5\", fontsize=12)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.legend(title=\"Experiment\", fontsize=10)\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.4)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results/summary_frontier.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"Saved → results/summary_frontier.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai-ws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}